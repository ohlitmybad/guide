name: Translate to French

on:
  push:
    branches: [ main, master ]
    paths:
      - 'locales/xx.json'

jobs:
  translate-to-french:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout guide repo
      uses: actions/checkout@v3
      with:
        token: ${{ secrets.PAT }}
        fetch-depth: 2  # To get the previous commit for comparison

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install openai beautifulsoup4 lxml

    - name: Update French Translation Based on Changes
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        cat > translate_changes.py << 'EOL'
        import json
        import os
        import time
        import sys
        import re
        import subprocess
        from bs4 import BeautifulSoup
        from openai import OpenAI

        def log_with_time(message):
            timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
            print(f"[{timestamp}] {message}")
            sys.stdout.flush()

        # Get old version of xx.json from the previous commit
        def get_previous_version():
            log_with_time("Getting previous version of xx.json")
            try:
                result = subprocess.run(
                    ["git", "show", "HEAD~1:locales/xx.json"],
                    capture_output=True, text=True, check=True
                )
                return json.loads(result.stdout)
            except Exception as e:
                log_with_time(f"Error getting previous version: {e}")
                log_with_time("This might be the first commit with xx.json")
                return None

        # Load current version of xx.json
        log_with_time("Loading current xx.json file")
        with open('locales/xx.json', 'r', encoding='utf-8') as file:
            current_data = json.load(file)

        # Try to load existing fr.json if it exists
        fr_data = {
            "common": {"nav": {"players": "Joueurs", "pro": "Pro", "teams": "Équipes"}},
            "meta": {"title": "DataMB | Guide", "description": "Explication des outils, des statistiques et de la méthodologie utilisés par DataMB"},
            "search": {"placeholder": "Cherchez une équipe ou joueur..."},
            "page-content": ""
        }
        
        try:
            with open('locales/fr.json', 'r', encoding='utf-8') as file:
                fr_data = json.load(file)
                log_with_time("Loaded existing fr.json file")
        except FileNotFoundError:
            log_with_time("No existing fr.json found, will create a new one")

        # Get the previous version of xx.json
        previous_data = get_previous_version()
        
        if not previous_data:
            log_with_time("No previous version found, will translate the entire content")
            previous_content = ""
        else:
            previous_content = previous_data.get('page-content', '')

        current_content = current_data.get('page-content', '')
        french_content = fr_data.get('page-content', '')

        if not current_content:
            log_with_time("No 'page-content' found in current xx.json")
            sys.exit(1)

        # Function to find the containing div for a specific text
        def find_containing_div(html_content, text_fragment):
            soup = BeautifulSoup(html_content, 'lxml')
            
            # Find all text nodes containing the fragment
            text_nodes = []
            for string in soup.strings:
                if text_fragment in string:
                    text_nodes.append(string)
            
            divs = []
            for text_node in text_nodes:
                # Find the closest containing div
                parent = text_node.parent
                div = None
                while parent and parent.name != 'div':
                    parent = parent.parent
                
                if parent and parent.name == 'div':
                    divs.append(str(parent))
            
            return divs

        # Initialize OpenAI client
        log_with_time("Initializing OpenAI API client")
        client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

        # Function to detect non-whitespace changes
        def detect_changes(old_text, new_text):
            # Remove whitespace from both texts for comparison
            old_normalized = re.sub(r'\s+', '', old_text)
            new_normalized = re.sub(r'\s+', '', new_text)
            
            if old_normalized != new_normalized:
                return True
            return False

        # Function to translate a specific div
        def translate_div(div_content):
            log_with_time(f"Translating div ({len(div_content)} characters)")
            
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Translate the following HTML div content from English to French. Maintain all HTML tags and structure intact. Use widely accepted French terminology, ensuring accuracy and natural flow. Output only the translated HTML, no explanations or additional text."},
                    {"role": "user", "content": div_content}
                ],
                temperature=0.7,
            )
            
            translated_div = response.choices[0].message.content
            
            # Clean up response if needed
            if translated_div.startswith("```html") and translated_div.endswith("```"):
                translated_div = translated_div[7:-3].strip()
            
            return translated_div

        # If this is the first run or there's no previous version, translate everything
        if not previous_content or not french_content:
            log_with_time("No previous English or French content - translating entire page content")
            
            # Parse the HTML to get all divs
            soup = BeautifulSoup(current_content, 'lxml')
            divs = soup.find_all('div')
            
            translated_divs = {}
            for i, div in enumerate(divs):
                div_str = str(div)
                div_id = div.get('id', f"div_{i}")
                log_with_time(f"Translating div {i+1}/{len(divs)} (id: {div_id})")
                translated_divs[div_str] = translate_div(div_str)
            
            # Replace all divs in the content
            translated_content = current_content
            for orig, translated in translated_divs.items():
                translated_content = translated_content.replace(orig, translated)
            
            fr_data['page-content'] = translated_content
        else:
            # Find differences between old and new content
            log_with_time("Detecting changes between previous and current content")
            
            # Initialize BeautifulSoup for both versions
            old_soup = BeautifulSoup(previous_content, 'lxml')
            new_soup = BeautifulSoup(current_content, 'lxml')
            fr_soup = BeautifulSoup(french_content, 'lxml')
            
            # Extract all divs and compare them
            old_divs = {div.get('id', str(i)): str(div) for i, div in enumerate(old_soup.find_all('div'))}
            new_divs = {div.get('id', str(i)): str(div) for i, div in enumerate(new_soup.find_all('div'))}
            
            # Identify divs that have changed
            changed_divs = []
            for div_id, new_div in new_divs.items():
                old_div = old_divs.get(div_id, '')
                if old_div and detect_changes(old_div, new_div):
                    log_with_time(f"Detected change in div with id/index: {div_id}")
                    changed_divs.append((div_id, new_div))
                elif not old_div:
                    log_with_time(f"New div found with id/index: {div_id}")
                    changed_divs.append((div_id, new_div))
            
            # Also check for deleted divs
            for div_id in old_divs:
                if div_id not in new_divs:
                    log_with_time(f"Div removed with id/index: {div_id}")
            
            # Translate changed divs
            translated_divs = {}
            for div_id, div_content in changed_divs:
                translated_divs[div_content] = translate_div(div_content)
            
            # Apply translations to the French content
            translated_content = current_content
            for orig, translated in translated_divs.items():
                translated_content = translated_content.replace(orig, translated)
            
            fr_data['page-content'] = translated_content
            
        # Write the updated French translation
        log_with_time("Writing updated fr.json file")
        os.makedirs('locales', exist_ok=True)
        with open('locales/fr.json', 'w', encoding='utf-8') as json_file:
            json.dump(fr_data, json_file, ensure_ascii=False, indent=2)
            
        log_with_time("Translation complete!")
        EOL

        python translate_changes.py

    - name: Commit and push French translation
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add locales/fr.json
        git diff --quiet && git diff --staged --quiet || git commit -m "Auto-translate changes in xx.json to French"
        git push

    - name: Checkout proguide repo
      uses: actions/checkout@v3
      with:
        repository: ohlitmybad/proguide
        path: proguide
        token: ${{ secrets.PAT }}

    - name: Commit and push proguide changes
      run: |
        cd proguide
        mkdir -p locales
        cp $GITHUB_WORKSPACE/locales/fr.json locales/
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add locales/fr.json
        git diff --quiet && git diff --staged --quiet || git commit -m "Auto-translate changes in xx.json to French"
        git push
