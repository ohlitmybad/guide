name: Translate to French

on:
  push:
    branches: [ main, master ]
    paths:
      - 'locales/xx.json'

jobs:
  translate-to-french:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout guide repo
      uses: actions/checkout@v3
      with:
        token: ${{ secrets.PAT }}

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install openai

    - name: Translate xx.json to French
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}  # Add your OpenAI API key to GitHub Secrets
      id: translate
      run: |
        cat > translate_to_french.py << 'EOL'
        import json
        import os
        import time
        import sys
        from openai import OpenAI  # Import the new OpenAI client
        import re

        # Function to log with timestamps
        def log_with_time(message):
            timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
            print(f"[{timestamp}] {message}")
            sys.stdout.flush()  # Force output to be written immediately

        start_time = time.time()
        log_with_time("Starting translation process")

        # Load the xx.json file
        log_with_time("Loading xx.json file")
        with open('locales/xx.json', 'r', encoding='utf-8') as file:
            en_data = json.load(file)

        # Extract the page-content
        log_with_time("Extracting page-content")
        page_content = en_data.get('page-content', '')
        if not page_content:
            raise Exception("No 'page-content' found in xx.json")

        # Initialize OpenAI API
        log_with_time("Initializing OpenAI API client")
        client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))  # Use the new client

        # Check for large content and process in batches if needed
        log_with_time(f"Content length: {len(page_content)} characters")
        
        # Process in chunks if the content is large (over 6000 characters)
        if len(page_content) > 6000:
            log_with_time("Content is large, processing in batches")
            
            # Split content into manageable chunks based on HTML structure
            # Look for major section divisions like </div> or </section> to find good split points
            chunks = []
            current_position = 0
            
            # Function to find a good split point
            def find_split_point(text, start_pos, chunk_size=6000):
                end_pos = min(start_pos + chunk_size, len(text))
                if end_pos >= len(text):
                    return len(text)
                
                # Look for end of section/div tags as good split points
                for marker in ['</section>', '</div>']:
                    # Find the last occurrence of the marker before our target position
                    marker_pos = text.rfind(marker, start_pos, end_pos)
                    if marker_pos != -1:
                        return marker_pos + len(marker)
                
                # If no good split point found, look for end of paragraph
                paragraph_end = text.rfind('</p>', start_pos, end_pos)
                if paragraph_end != -1:
                    return paragraph_end + 4
                
                # Fallback to sentence end
                for punct in ['. ', '! ', '? ']:
                    punct_pos = text.rfind(punct, start_pos, end_pos)
                    if punct_pos != -1:
                        return punct_pos + 2
                
                # Last resort: just split at chunk_size
                return end_pos
            
            # Split content into chunks
            while current_position < len(page_content):
                split_point = find_split_point(page_content, current_position)
                chunk = page_content[current_position:split_point]
                chunks.append(chunk)
                current_position = split_point
            
            log_with_time(f"Split content into {len(chunks)} chunks")
            
            # Translate each chunk separately
            translated_chunks = []
            for i, chunk in enumerate(chunks):
                log_with_time(f"Translating chunk {i+1}/{len(chunks)} ({len(chunk)} characters)")
                
                # Translate current chunk
                chunk_translation_start = time.time()
                response = client.chat.completions.create(
                    model="gpt-4o-mini",  # Use "gpt-4" or "gpt-3.5-turbo"
                    messages=[
                        {"role": "system", "content": "Translate the given JSON HTML content into French while keeping all JSON HTML tags and structure intact. Use widely accepted French football terminology, ensuring accuracy and natural flow. If uncertain about a term, verify its correct usage with online sources. Maintain authenticity and precision without altering the original meaning. Output only the translations of the following and that only, no extra text or markdown"},
                        {"role": "user", "content": chunk}
                    ],
                    temperature=0.7,
                )
                chunk_translation_end = time.time()
                
                log_with_time(f"Chunk {i+1} translated in {(chunk_translation_end - chunk_translation_start):.2f} seconds")
                
                # Get translated chunk
                translated_chunk = response.choices[0].message.content
                if translated_chunk.startswith("```json\n") and translated_chunk.endswith("\n```"):
                    translated_chunk = translated_chunk[7:-3].strip()
                
                translated_chunks.append(translated_chunk)
            
            # Combine all translated chunks
            log_with_time("Combining all translated chunks")
            translated_content = "".join(translated_chunks)
            
        else:
            # For smaller content, translate the whole thing at once
            log_with_time("Content is small enough to translate in one go")
            
            # Translate the content to French
            log_with_time("Sending translation request to OpenAI API")
            translation_start = time.time()
            response = client.chat.completions.create(
                model="gpt-4o-mini",  # Use "gpt-4" or "gpt-3.5-turbo"
                messages=[
                    {"role": "system", "content": "Translate the given JSON HTML content into French while keeping all JSON HTML tags and structure intact. Use widely accepted French football terminology, ensuring accuracy and natural flow. If uncertain about a term, verify its correct usage with online sources. Maintain authenticity and precision without altering the original meaning. Output only the translations of the following and that only, no extra text or markdown"},
                    {"role": "user", "content": page_content}
                ],
                temperature=0.7,
            )
            translation_end = time.time()
            log_with_time(f"Received response from OpenAI API after {(translation_end - translation_start):.2f} seconds")

            # Extract the translated content
            log_with_time("Extracting translated content from response")
            translated_content = response.choices[0].message.content
            
            if translated_content.startswith("```json\n") and translated_content.endswith("\n```"):
                log_with_time("Removing code block markers from response")
                translated_content = translated_content[7:-3].strip()  # Remove ```json\n and \n```

        # Create the fr.json file
        log_with_time("Creating fr.json file")
        fr_data = {"common": {"nav": {"players": "Joueurs","pro": "Pro","teams": "Équipes"}},"meta": {"title": "DataMB | Guide","description": "Explication des outils, des statistiques et de la méthodologie utilisés par DataMB"},"search": {"placeholder": "Cherchez une équipe ou joueur..."},"page-content": translated_content}
        os.makedirs('locales', exist_ok=True)
        with open('locales/fr.json', 'w', encoding='utf-8') as json_file:
            json.dump(fr_data, json_file, ensure_ascii=False, indent=2)

        total_time = time.time() - start_time
        log_with_time(f"Translated content saved to locales/fr.json. Total execution time: {total_time:.2f} seconds")
        EOL

        python translate_to_french.py

    - name: Commit and push French translation
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add locales/fr.json
        git diff --quiet && git diff --staged --quiet || git commit -m "Auto-translate xx.json to French"
        git push

    - name: Checkout proguide repo
      uses: actions/checkout@v3
      with:
        repository: ohlitmybad/proguide
        path: proguide
        token: ${{ secrets.PAT }}

    - name: Commit and push proguide changes
      run: |
        cd proguide
        mkdir -p locales
        cp $GITHUB_WORKSPACE/locales/fr.json locales/
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add index.html locales/fr.json
        git diff --quiet && git diff --staged --quiet || git commit -m "Auto-translate xx.json to French"
        git push
