name: Translate to Multiple Languages

on:
  push:
    branches: [ main, master ]
    paths:
      - 'locales/xx.json'

jobs:
  translate-to-multiple-languages:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout guide repo
      uses: actions/checkout@v3
      with:
        token: ${{ secrets.PAT }}
        fetch-depth: 2  # Fetch the last two commits

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install openai beautifulsoup4 lxml

    - name: Translate changes in xx.json to multiple languages
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      id: translate
      run: |
        cat > translate_to_languages.py << 'EOL'
        import json
        import os
        import time
        import sys
        import re
        import subprocess
        from bs4 import BeautifulSoup
        from openai import OpenAI
        import hashlib

        # Language configurations
        LANGUAGES = {
            'fr': {
                'name': 'French',
                'default_data': {
                    "common": {"nav": {"players": "Joueurs","pro": "Pro","teams": "Équipes"}},
                    "meta": {"title": "DataMB | Guide","description": "Explication des outils, des statistiques et de la méthodologie utilisés par DataMB"},
                    "search": {"placeholder": "Cherchez une équipe ou joueur..."}
                },
                'system_prompt': "Translate the given HTML content into French while keeping all HTML tags and structure intact. Use widely accepted French football terminology, ensuring accuracy and natural flow. If uncertain about a term, verify its correct usage with online sources. Maintain authenticity and precision without altering the original meaning. Output only the translation, no extra text or markdown."
            },
            'tr': {
                'name': 'Turkish',
                'default_data': {
                    "common": {"nav": {"players": "Oyuncular","pro": "Pro","teams": "Takımlar"}},
                    "meta": {"title": "DataMB | Rehber","description": "DataMB tarafından kullanılan araçlar, istatistikler ve metodolojinin açıklaması"},
                    "search": {"placeholder": "Takım veya oyuncu arayın..."}
                },
                'system_prompt': "Translate the given HTML content into Turkish while keeping all HTML tags and structure intact. Use widely accepted Turkish football terminology, ensuring accuracy and natural flow. If uncertain about a term, verify its correct usage with online sources. Maintain authenticity and precision without altering the original meaning. Output only the translation, no extra text or markdown."
            },
            'es': {
                'name': 'Spanish',
                'default_data': {
                    "common": {"nav": {"players": "Jugadores","pro": "Pro","teams": "Equipos"}},
                    "meta": {"title": "DataMB | Guía","description": "Explicación de las herramientas, estadísticas y metodología utilizadas por DataMB"},
                    "search": {"placeholder": "Buscar equipo o jugador..."}
                },
                'system_prompt': "Translate the given HTML content into Spanish while keeping all HTML tags and structure intact. Use widely accepted Spanish football terminology, ensuring accuracy and natural flow. If uncertain about a term, verify its correct usage with online sources. Maintain authenticity and precision without altering the original meaning. Output only the translation, no extra text or markdown."
            }
        }

        # Function to log with timestamps
        def log_with_time(message):
            timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
            print(f"[{timestamp}] {message}")
            sys.stdout.flush()

        def get_previous_content():
            """Get the content from the previous commit of xx.json"""
            try:
                # Get the previous commit hash for the file
                result = subprocess.run(
                    ["git", "log", "-n", "2", "--pretty=format:%H", "locales/xx.json"],
                    capture_output=True, text=True, check=True
                )
                commit_hashes = result.stdout.strip().split('\n')
                
                if len(commit_hashes) < 2:
                    log_with_time("No previous commit found, using empty JSON")
                    return {"page-content": ""}
                
                # Get the content from the previous commit
                prev_commit = commit_hashes[1]  # The second most recent commit
                result = subprocess.run(
                    ["git", "show", f"{prev_commit}:locales/xx.json"],
                    capture_output=True, text=True, check=True
                )
                return json.loads(result.stdout)
            except Exception as e:
                log_with_time(f"Error getting previous content: {e}")
                # If there's an error, assume it's a new file
                return {"page-content": ""}

        def extract_sections(html_content):
            """Parse HTML and extract sections with their identifiers"""
            soup = BeautifulSoup(html_content, 'lxml')
            sections = {}
            section_hierarchy = {}  # Track parent-child relationships
            
            # Look for elements with id attributes - these are easiest to match
            elements_with_id = soup.select('[id]')
            for element in elements_with_id:
                section_id = element.get('id')
                sections[section_id] = str(element)
                
                # Check if this element is inside another element with an id
                for parent in element.parents:
                    if parent.get('id') and parent.get('id') in sections:
                        section_hierarchy[section_id] = parent.get('id')
                        break
            
            # If no elements with id found or very few, extract by major sections
            if len(sections) < 3:
                # Clear sections to avoid mixing strategies
                sections = {}
                section_hierarchy = {}
                # Use HTML structure fingerprinting for sections
                for i, section in enumerate(soup.find_all(['section', 'div', 'article'])):
                    # Create a fingerprint based on tag structure, classes, and attributes
                    fingerprint = create_element_fingerprint(section)
                    section_id = f"fp_{fingerprint}"
                    sections[section_id] = str(section)
                    
                    # Check for parent-child relationships
                    for parent in section.parents:
                        if parent.name in ['section', 'div', 'article']:
                            parent_fingerprint = create_element_fingerprint(parent)
                            parent_id = f"fp_{parent_fingerprint}"
                            if parent_id in sections:
                                section_hierarchy[section_id] = parent_id
                                break
            
            return sections, section_hierarchy
            
        def create_element_fingerprint(element):
            """Creates a structural fingerprint of an element that's language-independent
            Based on tag name, classes, and structure rather than content"""
            # Start with the tag name
            fingerprint = element.name or "unknown"
            
            # Add classes if present (sorted to ensure consistency)
            classes = sorted(element.get('class', []))
            if classes:
                fingerprint += "_" + "_".join(classes)
                
            # Add data attributes as these are usually structural
            data_attrs = []
            for attr_name, value in element.attrs.items():
                if attr_name.startswith('data-'):
                    data_attrs.append(f"{attr_name}={value}")
            if data_attrs:
                fingerprint += "_" + "_".join(sorted(data_attrs))
                
            # Add child structure info (just tag counts to keep it simple)
            child_tags = {}
            for child in element.find_all(recursive=False):
                tag = child.name
                child_tags[tag] = child_tags.get(tag, 0) + 1
                
            # Add child tag counts to fingerprint
            if child_tags:
                child_info = []
                for tag, count in sorted(child_tags.items()):
                    child_info.append(f"{tag}{count}")
                fingerprint += "_" + "".join(child_info)
                
            # Create a hash of the fingerprint to keep it shorter
            return hashlib.md5(fingerprint.encode('utf-8')).hexdigest()[:12]

        def compute_section_hash(section_content):
            """Compute a hash for each section to track changes"""
            return hashlib.md5(section_content.encode('utf-8')).hexdigest()

        def translate_section(client, section_content, system_prompt):
            """Translate a single section using OpenAI API"""
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": section_content}
                ],
                temperature=0.7,
            )
            translated_content = response.choices[0].message.content
            
            # Clean up any markdown code blocks if present
            if translated_content.startswith("```") and "```" in translated_content:
                translated_content = re.sub(r"```[a-z]*\n", "", translated_content, 1)
                translated_content = translated_content.rstrip("\n```")
            
            return translated_content

        def process_language(lang_code, lang_config, current_sections, current_hierarchy, 
                           previous_sections, current_content, sections_to_translate, client):
            """Process translation for a single language"""
            log_with_time(f"Processing {lang_config['name']} translation")
            
            # Load existing translation if available
            lang_file = f'locales/{lang_code}.json'
            lang_data = lang_config['default_data'].copy()
            
            try:
                with open(lang_file, 'r', encoding='utf-8') as file:
                    lang_data = json.load(file)
                    log_with_time(f"Loaded existing {lang_code}.json")
            except FileNotFoundError:
                log_with_time(f"No existing {lang_code}.json found, will create new file")

            # If no sections to translate, we're done for this language
            if not sections_to_translate:
                log_with_time(f"No changes detected that require translation for {lang_config['name']}")
                return

            # Load existing content if available
            existing_content = lang_data.get('page-content', '')
            existing_sections = {}
            existing_hierarchy = {}
            if existing_content:
                existing_sections, existing_hierarchy = extract_sections(existing_content)
                log_with_time(f"Extracted {len(existing_sections)} sections from existing {lang_config['name']} content")

            # Translate changed sections
            log_with_time(f"Translating {len(sections_to_translate)} changed sections for {lang_config['name']}")
            translated_sections = {}
            
            for i, (section_id, content) in enumerate(sections_to_translate.items()):
                log_with_time(f"Translating {lang_config['name']} section {i+1}/{len(sections_to_translate)}: {section_id}")
                translation_start = time.time()
                
                translated_section = translate_section(client, content, lang_config['system_prompt'])
                
                translation_end = time.time()
                log_with_time(f"{lang_config['name']} section {section_id} translated in {(translation_end - translation_start):.2f} seconds")
                
                translated_sections[section_id] = translated_section

            # Construct the final content by replacing only the changed sections
            log_with_time(f"Constructing final {lang_config['name']} content")
            final_content = current_content  # Default fallback
            
            # If we have existing content, use it as a base and update only changed sections
            if existing_content:
                final_content = existing_content
                soup = BeautifulSoup(final_content, 'lxml')
                
                # Create a mapping from English section fingerprints to corresponding elements
                element_map = {}
                
                # First, map by ID if available
                for element in soup.select('[id]'):
                    element_map[element['id']] = element
                    
                # Then map by structural fingerprint for non-ID elements
                for element in soup.find_all(['section', 'div', 'article']):
                    if not element.get('id'):
                        fingerprint = create_element_fingerprint(element)
                        element_map[f"fp_{fingerprint}"] = element
                
                log_with_time(f"Created {lang_config['name']} element map with {len(element_map)} entries")
                
                # Statistics for logging
                sections_matched = 0
                sections_not_matched = 0
                
                # For each translated section, find its corresponding element
                for section_id, translated_content in translated_sections.items():
                    # Try to find the element using our mapping
                    element = element_map.get(section_id)
                    
                    if element:
                        # We found a direct match by ID or fingerprint
                        sections_matched += 1
                        log_with_time(f"Found direct match for {lang_config['name']} section {section_id}")
                        
                        # Create a new element from the translated content
                        new_soup = BeautifulSoup(translated_content, 'lxml')
                        if new_soup.contents:
                            new_element = new_soup.contents[0]
                            # Preserve the ID from the original element if it exists
                            if element.get('id') and not new_element.get('id'):
                                new_element['id'] = element['id']
                            element.replace_with(new_element)
                        else:
                            log_with_time(f"Warning: Translated content for {lang_config['name']} {section_id} could not be parsed")
                    
                    else:
                        # No direct match found, try alternative matching strategies
                        sections_not_matched += 1
                        log_with_time(f"No direct match found for {lang_config['name']} section {section_id}, trying alternative strategies")
                        
                        # Try to find by structural similarity (similar to original logic)
                        if section_id.startswith("section_") or section_id.startswith("fp_"):
                            en_section = None
                            en_soup = BeautifulSoup(current_content, 'lxml')
                            
                            # Try to find the original English element that was translated
                            if section_id.startswith("section_"):
                                index = int(section_id.split("_")[1])
                                en_elements = en_soup.find_all(['section', 'div', 'article'])
                                if 0 <= index < len(en_elements):
                                    en_section = en_elements[index]
                            elif section_id.startswith("fp_"):
                                fingerprint = section_id.split("_", 1)[1]
                                for element_check in en_soup.find_all(['section', 'div', 'article']):
                                    if create_element_fingerprint(element_check) == fingerprint:
                                        en_section = element_check
                                        break
                            
                            if en_section:
                                # Get the tag name, classes, and approximate position
                                tag_name = en_section.name
                                classes = en_section.get('class', [])
                                
                                # Find all elements with same tag and classes
                                potential_matches = []
                                
                                for elem in soup.find_all(tag_name):
                                    elem_classes = elem.get('class', [])
                                    class_overlap = set(classes) & set(elem_classes)
                                    
                                    if (not classes and not elem_classes) or class_overlap:
                                        # Compute structural similarity score
                                        en_structure = [child.name for child in en_section.find_all(recursive=False)]
                                        elem_structure = [child.name for child in elem.find_all(recursive=False)]
                                        
                                        # Simple similarity measure: count of common child tags
                                        similarity = len(set(en_structure) & set(elem_structure))
                                        
                                        potential_matches.append((elem, similarity))
                                
                                # Sort by similarity score (highest first)
                                potential_matches.sort(key=lambda x: x[1], reverse=True)
                                
                                if potential_matches:
                                    # Use the best match
                                    best_match = potential_matches[0][0]
                                    log_with_time(f"Found best structural match for {lang_config['name']} {section_id} with similarity score {potential_matches[0][1]}")
                                    
                                    # Create a new element from the translated content
                                    new_soup = BeautifulSoup(translated_content, 'lxml')
                                    if new_soup.contents:
                                        new_element = new_soup.contents[0]
                                        # Preserve the ID and classes from the original element
                                        if best_match.get('id') and not new_element.get('id'):
                                            new_element['id'] = best_match['id']
                                        best_match.replace_with(new_element)
                                        sections_matched += 1
                                        sections_not_matched -= 1
                                    else:
                                        log_with_time(f"Warning: Translated content for {lang_config['name']} {section_id} could not be parsed")
                
                log_with_time(f"{lang_config['name']} section matching summary: {sections_matched} matched, {sections_not_matched} not matched")
                
                # Update the final content
                final_content = str(soup)
            else:
                # For a completely new translation, translate everything
                log_with_time(f"No existing {lang_config['name']} content to update, using full translation")
                translation_start = time.time()
                
                response = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": lang_config['system_prompt']},
                        {"role": "user", "content": current_content}
                    ],
                    temperature=0.7,
                )
                
                final_content = response.choices[0].message.content
                if final_content.startswith("```") and "```" in final_content:
                    final_content = re.sub(r"```[a-z]*\n", "", final_content, 1)
                    final_content = final_content.rstrip("\n```")
                    
                translation_end = time.time()
                log_with_time(f"Full {lang_config['name']} translation completed in {(translation_end - translation_start):.2f} seconds")

            # Save the language file
            log_with_time(f"Creating {lang_code}.json file")
            lang_data["page-content"] = final_content
            os.makedirs('locales', exist_ok=True)
            with open(lang_file, 'w', encoding='utf-8') as json_file:
                json.dump(lang_data, json_file, ensure_ascii=False, indent=2)
            
            log_with_time(f"{lang_config['name']} translation completed and saved to {lang_file}")

        # Main execution starts here
        start_time = time.time()
        log_with_time("Starting multi-language translation process")

        # Load the current xx.json file
        log_with_time("Loading current xx.json file")
        try:
            with open('locales/xx.json', 'r', encoding='utf-8') as file:
                current_data = json.load(file)
        except Exception as e:
            log_with_time(f"Error loading xx.json: {e}")
            sys.exit(1)

        # Get the content from the previous commit
        log_with_time("Retrieving content from previous commit")
        previous_data = get_previous_content()

        # Extract the page-content from both versions
        current_content = current_data.get('page-content', '')
        previous_content = previous_data.get('page-content', '')
        
        if not current_content:
            log_with_time("No 'page-content' found in current xx.json")
            sys.exit(1)

        # Extract sections from both versions
        log_with_time("Parsing HTML and extracting sections")
        current_sections, current_hierarchy = extract_sections(current_content)
        previous_sections, previous_hierarchy = extract_sections(previous_content)
        
        log_with_time(f"Extracted {len(current_sections)} sections from current content")
        log_with_time(f"Detected {len(current_hierarchy)} parent-child relationships between sections")

        # Identify changed sections by comparing hashes
        log_with_time("Identifying changed sections")
        sections_to_translate = {}
        for section_id, content in current_sections.items():
            current_hash = compute_section_hash(content)
            prev_content = previous_sections.get(section_id, '')
            prev_hash = compute_section_hash(prev_content) if prev_content else ''
            
            if current_hash != prev_hash:
                # Check if this section's parent has already been marked for translation
                parent_id = current_hierarchy.get(section_id)
                if parent_id and parent_id in sections_to_translate:
                    log_with_time(f"Section {section_id} changed but parent {parent_id} already marked for translation, skipping to avoid duplication")
                    continue
                
                log_with_time(f"Change detected in section {section_id}, will translate")
                sections_to_translate[section_id] = content
            else:
                log_with_time(f"No change in section {section_id}, skipping translation")

        # Check if any language files are missing - if so, translate everything
        missing_languages = []
        for lang_code in LANGUAGES.keys():
            if not os.path.exists(f'locales/{lang_code}.json'):
                missing_languages.append(lang_code)
        
        if missing_languages or not previous_content:
            log_with_time(f"Missing language files {missing_languages} or complete content change, translating entire content")
            
            # Filter out any child sections if their parent is already included
            sections_to_translate = {
                section_id: content 
                for section_id, content in current_sections.items() 
                if section_id not in current_hierarchy or current_hierarchy[section_id] not in current_sections
            }
            
            log_with_time(f"After filtering nested sections, translating {len(sections_to_translate)} top-level sections")

        # If no sections to translate, we're done
        if not sections_to_translate:
            log_with_time("No changes detected that require translation")
            sys.exit(0)

        # Initialize OpenAI API
        log_with_time("Initializing OpenAI API client")
        client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

        # Process each language
        for lang_code, lang_config in LANGUAGES.items():
            try:
                process_language(lang_code, lang_config, current_sections, current_hierarchy,
                               previous_sections, current_content, sections_to_translate, client)
            except Exception as e:
                log_with_time(f"Error processing {lang_config['name']}: {e}")
                # Continue with other languages even if one fails

        # Save a cache of the current content for future reference
        with open('locales/xx_previous.json', 'w', encoding='utf-8') as cache_file:
            json.dump(current_data, cache_file, ensure_ascii=False, indent=2)

        total_time = time.time() - start_time
        log_with_time(f"Multi-language translation completed. Total execution time: {total_time:.2f} seconds")
        EOL

        python translate_to_languages.py

    - name: Commit and push translations
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add locales/fr.json locales/tr.json locales/es.json locales/xx_previous.json
        git diff --quiet && git diff --staged --quiet || git commit -m "Auto-translate xx.json to French, Turkish, and Spanish"
        git push

    - name: Checkout proguide repo
      uses: actions/checkout@v3
      with:
        repository: ohlitmybad/proguide
        path: proguide
        token: ${{ secrets.PAT }}

    - name: Commit and push proguide changes
      run: |
        cd proguide
        mkdir -p locales
        cp $GITHUB_WORKSPACE/locales/fr.json locales/
        cp $GITHUB_WORKSPACE/locales/tr.json locales/
        cp $GITHUB_WORKSPACE/locales/es.json locales/
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add locales/fr.json locales/tr.json locales/es.json
        git diff --quiet && git diff --staged --quiet || git commit -m "Auto-translate xx.json to French, Turkish, and Spanish"
        git push
